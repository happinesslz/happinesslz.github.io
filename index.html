<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhe Liu</title>

  <meta name="author" content="Zhe Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Zhe Liu
                  </p>
                  <p>I am a fourth-year Ph.D student at Huazhong University of Science and Technology (HUST), supervised by Prof. <a
                      href="https://xbai.vlrlab.net/">Xiang Bai</a>.
                  </p>
                  <p>
                    Currently, my research focuses on 3D Perception in Autonomous Driving: 3D object Detection, 
3D multi-object Tracking, Multi-modal representation learning, 3D Point Cloud Analysis, etc. In the future, I will pay more attention to unified representation learning of multiple modalities, training a large 3D model by effectively unifying different autonomous datasets (e.g., nuScenes, KITTI, Waymo, ONCE, Argoverse), and Combination of 3D perception and large language Model for building 3D world model. 
                  </p>
                  <p style="text-align:center">
                    <a href="zheliu1994@hust.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="data/zhe_liu_cv_latex.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=yprv7EsAAAAJ&hl=zh-CN&oi=sra">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/happinesslz/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zheliu.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/zheliu.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <p>
                    Jul 2024: Two papers (SEED, OPEN) are accepted by ECCV 2024.
                  </p>
                  <p>
                    Sep 2023: QTNet is accepted by NeurIPS 2023.
                  </p>
                  <p>
                    Nov 2022: StereoDistill is accepted by AAAI 2023.
                  </p>
                  <p>
                    Dec 2022: EPNet++ is accepted by T-PAMI 2022.
                  </p>
                  <p>
                    Jul 2020: EPNet is accepted by ECCV 2020.
                  </p>
                  <p>
                    Nov 2019: TANet is accepted by AAAI 2020 as an <strong>Oral</strong> presentation.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/seed.png' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">SEED: A Simple and Effective 3D DETR in Point Clouds</span>
                  </a>
                  <br>
                  <strong>Zhe Liu*</strong>, Jinghua Hou*, Xiaoqing Ye, Tong Wang, Jingdong Wang, Xiang Bai
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <!-- <a
                    href="https://openreview.net/pdf?id=gySmwdmVDF">paper</a> /
                  <a href="https://github.com/AlmoonYsl/QTNet">code</a> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/open.jpg' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection</span>
                  </a>
                  <br>
                  Jinghua Hou, Tong Wang, Xiaoqing Ye, <strong>Zhe Liu</strong>, Shi Gong, Xiao Tan, Errui Ding, Jingdong Wang, Xiang Bai
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <!-- <a
                    href="https://openreview.net/pdf?id=gySmwdmVDF">paper</a> /
                  <a href="https://github.com/AlmoonYsl/QTNet">code</a> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/qtnet.png' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">Query-based Temporal Fusion with Explicit Motion for 3D Object Detection</span>
                  </a>
                  <br>
                  Jinghua Hou*, <strong>Zhe Liu*</strong>, Dingkang Liang, Zhikang Zou, Xiaoqing Ye, Xiang Bai
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a
                    href="https://openreview.net/pdf?id=gySmwdmVDF">paper</a> /
                  <a href="https://github.com/AlmoonYsl/QTNet">code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/wss3d.jpg' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection</span>
                  </a>
                  <br>
                  Dingyuan Zhang*, Dingkang Liang*, Zhikang Zou*, Jingyu Li, Xiaoqing Ye, <strong>Zhe Liu</strong>, Xiao Tan, Xiang Bai
                  <br>
                  <em>ICCV</em>, 2023
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.pdf">paper</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/fbmnet.png' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">Multi-Modal 3D Object Detection by Box Matching</span>
                  </a>
                  <br>
                  <strong>Zhe Liu</strong>, Xiaoqing Ye, Zhikang Zou, Xinwei He, Xiao Tan, Errui Ding, Jingdong Wang, Xiang Bai
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a
                    href="https://arxiv.org/pdf/2305.07713">paper</a> /
                  <a href="https://github.com/happinesslz/FBMNet">code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/dds3d.png' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">DDS3D: Dense Pseudo-Labels with Dynamic Threshold for Semi-Supervised 3D Object Detection</span>
                  </a>
                  <br>
                  Jingyu Li*, <strong>Zhe Liu*</strong>, Jinghua Hou, Dingkang Liang
                  <br>
                  <em>ICRA</em>, 2023
                  <br>
                  <a
                    href="https://arxiv.org/pdf/2303.05079">paper</a> /
                  <a href="https://github.com/Whale-ice/DDS3D">code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/stereo_distill.png' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection</span>
                  </a>
                  <br>
                  <strong>Zhe Liu</strong>, Xiaoqing Ye, Xiao Tan, Errui Ding, Xiang Bai
                  <br>
                  <em>AAAI</em>, 2023
                  <br>
                  <a
                    href="https://ojs.aaai.org/index.php/AAAI/article/download/25268/25040">paper</a>
                  <!-- <a href="https://github.com/AlmoonYsl/QTNet">code</a> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/epnet++.png' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection</span>
                  </a>
                  <br>
                  <strong>Zhe Liu</strong>, Tengteng Huang, Bingling Li, Xiwu Chen, Xi Wang, Xiang Bai
                  <br>
                  <em>T-PAMI</em>, 2022
                  <br>
                  <a
                    href="https://arxiv.org/pdf/2112.11088">paper</a> /
                  <a href="https://github.com/happinesslz/EPNetV2">code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/epnet.png' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection</span>
                  </a>
                  <br>
                  Tengteng Huang*, <strong>Zhe Liu*</strong>, Xiwu Chen, Xiang Bai
                  <br>
                  <em>ECCV</em>, 2020
                  <br>
                  <a
                    href="https://arxiv.org/pdf/2007.08856">paper</a> /
                  <a href="https://github.com/happinesslz/EPNet">code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/tanet.jpg' width="160">
                </td>
                <td style="padding:5px;width:75%;vertical-align:middle">
                  <a
                    href="https://happinesslz.github.io">
                    <span class="papertitle">TANet: Robust 3D Object Detection from Point Clouds with Triple Attention</span>
                  </a>
                  <br>
                  <strong>Zhe Liu</strong>, Xin Zhao, Tengteng Huang, Ruolan Hu, Yu Zhou, Xiang Bai
                  <br>
                  <em>AAAI</em>, 2020, <strong>Oral</strong>
                  <br>
                  <a
                    href="https://arxiv.org/pdf/1912.05163">paper</a> /
                  <a href="https://github.com/happinesslz/TANet">code</a>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Activities</h2>
                  <p>
                    Reviewer: CVPR, ICCV, ECCV, AAAI, NeurIPS, T-PAMI.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's
                    website</a>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>